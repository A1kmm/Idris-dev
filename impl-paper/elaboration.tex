\newcommand{\ttinterp}[1]{\mathcal{E}\interp{#1}}

\section{Elaborating \Idris{}}

An \Idris{} program consists of a series of declarations --- data types, functions,
type classes and instances. In this section, we describe how these high level declarations
are translated into a \TT{} program consisting of inductive families and pattern matching
function definitions. We will need to work at the \remph{declaration} level, and at
the \remph{expression} level, defining the following meta-operations
which together constitute an algorithm for elaborating \Idris{}
programs to \TT{}.

\begin{itemize}
\item $\ttinterp{\cdot}$, which builds a \TT{} expression from an \Idris{} expression.
\item $\MO{Elab}$, which processes a top level \Idris{} declaration by generating
one or more \TT{} declarations.
\item $\MO{TTDecl}$, which adds a top level \TT{} declaration.
\end{itemize}


\subsection{The Development Calculus \TTdev}

We build \TT{} expressions by using high level \Idris{} expressions to
direct a tactic based theorem prover, which builds the \TT{} expressions
step by step, by refinement. In order to build expressions in this way,
the type theory needs to support
\remph{incomplete} terms, and a method for term construction. 
To achieve this, we extend \TT{} with \remph{holes},
calling the extended calculus \TTdev{}.
Holes stand for the parts of programs which have not yet been
instantiated; this largely follows the \Oleg{} development
calculus~\cite{McBride1999}.

The basic idea is to extend the syntax for binders with a \remph{hole}
binding and a \remph{guess} binding. 
These extensions are given in Figure \ref{ttdev}.
The \remph{guess} binding is
similar to a $\LET$ binding, but without any computational force,
i.e. there are no reduction rules for guess bindings. 
Using binders to represent holes is useful in a dependently typed setting since
one value may determine another. Attaching a guess to a binder ensures that
instantiating one such value also instantiates all of its dependencies. The
typing rules for binders ensure that no $?$ bindings leak into types.

\FFIG{
\AR{
\vb ::= \ldots 
 \:\mid\: \hole{\vx}{\vt} \:\:(\mbox{hole binding}) \:\:
 \:\mid\: \guess{\vx}{\vt}{\vt} \:\:(\mbox{guess})
\medskip\\
\Rule{
\Gamma;\hole{\vx}{\vS}\proves\ve\Hab\vT
}
{
\Gamma\proves\hole{\vx}{\vS}\SC\ve\Hab\vT
}
\hspace*{0.1cm}\vx\not\in\vT
\hspace*{0.1in}\mathsf{Hole}
\hg
\Rule{
\Gamma;\guess{\vx}{\vS}{\ve_1}\proves\ve_2\Hab\vT
}
{
\Gamma\proves\guess{\vx}{\vS}{\ve_1}\SC\ve_2\Hab\vT
}
\hspace*{0.1cm}\vx\not\in\vT
\hspace*{0.1in}\mathsf{Guess}
}
}
{\TTdev{} extensions}
{ttdev}


\subsection{Proof State}

A proof state is a tuple, $(\vC, \Delta, \ve, \vQ)$, containing:

\begin{itemize}
\item A global context, $\vC$, containing pattern matching definitions and their types
\item A local context, $\Delta$, containing pattern bindings
\item A proof term, $\ve$, in \TTdev{}
\item A hole queue, $\vQ$
%\item \remph{Deferred} definitions, $\vD$, for introducing global metavariables
\end{itemize}

The \remph{hole queue} is a priority queue of names of hole and guess binders 
$\langle\vx_1,\vx_2,\ldots,\vx_n\rangle$
in the proof term ---
we ensure that each bound name is unique. Holes essentially refer to \remph{sub goals}
in the proof.
When this queue is empty, the proof term is complete.
Creating a \TT{} expression from an \Idris{} expresson involves creating
a new proof state, with an empty proof term, and using the high level definition
to direct the building of a final proof state, with a complete proof term.

In the implementation, the proof state is captured in an elaboration monad,
\texttt{Elab}, which includes various operations for querying and updating
the proof state, manipulating terms, generating fresh names, etc. However, we will
describe \Idris{} elaboration in terms of meta-operations on the proof state,
in order to capture the essence of the elaboration process without being distracted
by implementation details. These meta-operations include: 

\begin{itemize}
\item \demph{Queries} which retrieve values from the proof state, without modifying
the state. For example, we can:
\begin{itemize}
\item Get the type of the current sub goal
\item Retrieve the local context $\Gamma$ at the current sub goal
\item Type check or normalise a term relative to $\Gamma$
\end{itemize}
\item \demph{Unification}, which unifies two terms (potentially solving sub goals) 
relative to $\Gamma$
\item \demph{Tactics} which update the proof term. Tactics operate on the sub term
at the binder specified by the head of the hole queue $\vQ$.
\item \demph{Focussing} on a specific sub goal, which brings a different sub goal to the
head of the hole queue.
%\item \demph{Deferring} a sub goal, which adds a new definition to the global context
%$\vC$ which solves the sub goal.
\end{itemize}

Elaboration of an \Idris{} expression involves creating a new proof state, running
a series of tactics to build a complete proof term, then retrieving and \remph{rechecking}
the final proof term, which must be a \TT{} program (i.e. does not contain any of the
\TTdev{} extensions). We call a sub-term which contains no hole or guess bindings 
\demph{pure}. Although a pure term does not containg hole or guess bindings, it may
neverthless \remph{refer} to hole- or guess-bound variables.

We initialise a proof state with the $\MO{NewProof}$ operation. Given a global
context $\vC$, $\MO{NewProof}\:\vt$ sets up the proof state as:

\DM{
(\vC, \cdot, \hole{\vx}{\vt}\SC\vx, \langle\vx\rangle)
}

The local context is initially empty, and the initial hole queue is the $\vx$ standing for
the entire expression. We can reset the proof term with the $\MO{NewTerm}$ operation.
In an existing proof state $(\vC, \Delta, \ve, \vQ)$,
$\MO{NewTerm}\:\vt$ discards the proof term and hole queue, and
updates the proof state to:

\DM{
(\vC, \Delta, \hole{\vx}{\vt}\SC\vx, \langle\vx\rangle)
}


This allows us in particular to use pattern bindings from the left hand side of a pattern
matching definition in the term on the right hand side.

\subsection{System State}

The system state is a tuple, $(\vC,\vA,\vI)$, containing:

\begin{itemize}
\item A global context, $\vC$, containing pattern matching definitions and their types
\item Implicit arguments, $\vA$, recording which arguments are implicit for each global name
\item Type class instances, $\vI$, containing dictionaries for type classes
\end{itemize}

In the implementation, the system state is captured in a monad, \texttt{Idris}, and
includes additional information such as syntax overloadings,
command line options, and optimisations, which do not concern us here. Elaboration
of expressions requires access to the system state in particular in order to expand
implicit arguments and resolve type classes. 

For each global name, $\vA$ records whether its arguments are explicit, implicit,
or type class constraints.  For example, recall the declaration
of \texttt{vAdd}:

\begin{SaveVerbatim}{vAddImpT}

vAdd : Num a => Vect a n -> Vect a n -> Vect a n

\end{SaveVerbatim}
\useverb{vAddImpT} 

\noindent
Written in full, and giving each argument an explicit name, we get the
type declaration:

\begin{SaveVerbatim}{vAddImpT}

vAdd : (a : _) -> (n : _) -> (c : Num a) -> 
       (xs : Vect a n) -> (ys : Vect a n) -> Vect a n

\end{SaveVerbatim}
\useverb{vAddImpT} 

\noindent
For \tFN{vAdd}, we record that \texttt{a} and \texttt{n} are implicit, 
\texttt{c} is a constraint, and \texttt{xs} and \texttt{ys} are explicit. When
the elaborator encounters an application of \tFN{vAdd}, it knows that unless these arguments
are given explicitly, the application must be expanded.

\newcommand{\Check}{\MO{Check}_\Gamma}
\newcommand{\Eval}{\MO{Normalise}_\Gamma}
\newcommand{\Unify}{\MO{Unify}_\Gamma}

\subsection{Tactics}

% Meta-operations Check, Normalise, Unify 
In order to build \TT{} expressions from \Idris{} programs, we define a collection
of meta-operations for querying and modifying the proof state. Meta-operations
may have side-effects including failure, or updating the proof state. We have the following
primitive meta-operations:

\begin{itemize}
\item $\MO{Focus}\:\vn$, which moves $\vn$ to the head of the hole queue.
\item $\MO{Unfocus}$, which moves the current hole to the back of the hole queue.
\item $\Check\:\ve$, which type checks an expression $\ve$ relative to a context
$\Gamma$, returning its type.
$\MO{Check}$ will fail
if the expression is not well-typed.
\item $\Eval\:\ve$, which evaluates a well-typed expression $\ve$ relative to a context 
$\Gamma$, returning its normal form.
\item 
$\Unify\:\ve_1\:\ve_2$, 
which unifies $\ve_1$ and $\ve_2$ by finding the values with which holes must be instantiated
for $\ve_1$ and $\ve_2$ to be convertible relative to $\Gamma$
(i.e. for $\Gamma\proves\ve_1\converts\ve_2$ to hold). $\MO{Unify}$ will fail
if it cannot find such values. If successful, $\MO{Unify}$ will update the proof state.
\end{itemize}

\remph{Tactics} are specifically meta-operations which operate on the sub-term given
by the hole at the head of the hole queue in the proof state. They take the following form:

\DM{
\PA{\A\A}{
\MO{Tactic}_\Gamma & \:\vec{\VV{args}} & \:\vt & \MoRet{\vt'}
}
}

A tactic takes a sequence of zero or more arguments $\vec{\VV{args}}$ followed
by the sub-term $\vt$ on which it is operating. It runs relative to a context
$\Gamma$ which contains all the bindings and pattern bindings in scope at that
point in the term. The sub-term $\vt$ will either be a hole binding
$\hole{\vx}{\vT}\SC\ve$ or a guess binding $\guess{\vx}{\vT}{\vv}\SC\ve$. The
tactic returns a new term $\vt'$ which can take any form, provided it is
well-typed, with a type convertible to the type of $\vt$. 
Tactics may also have the side effect of updating the proof state,
therefore we will describe tactics in a pseudo-code with $\RW{do}$ notation.

We define a set of primitive tactics: $\MO{Claim}$, $\MO{Fill}$ and $\MO{Solve}$
which are used to create and destroy holes; and $\MO{Lambda}$, $\MO{Pi}$, $\MO{Let}$
and $\MO{Attack}$ which are used to create binders.

\subsubsection{Creating and destroying holes}

The $\MO{Claim}$ tactic, given a name and a type, adds a new hole binding in
the scope of the current goal $\vx$, adding the new binding to the hole queue, but
keeping $\vx$ at the head:

\DM{
\PA{\A\A}{
\MO{Claim}_\Gamma & (\vy \Hab\vS) & (\hole{\vx}{\vT}\SC\ve) & 
   \MoRet{\RW{return}\:\hole{\vx}{\vT}\SC\hole{\vy}{\vS}\SC\ve} \\
}
}

An obvious difficulty is in ensuring that names are unique throughout a proof term.
In the implementation, we ensure that any hole created by the $\MO{Claim}$ tactic
has a unique name. In this paper, we will assume that all names are fresh.

The $\MO{Fill}$ tactic, given a value $\vv$, attempts to solve the current goal
with $\vv$, creating a guess binding in its place. $\MO{Fill}$ attempts to
solve other holes by unifying the expected type of $\vx$ with the type of $\vv$:

\DM{
\PA{\A\A}{
\MO{Fill}_\Gamma & \vv & (\hole{\vx}{\vT}\SC\ve) & 
   \MoRet{\RW{do}\:\AR{
   \vT' \leftarrow \Check\:\vv\\
   \Unify\:\vT\:\vT'\\
   \RW{return}\:\guess{\vx}{\vT}{\vv'}\SC\ve}
   } \\
}
}

\noindent
For example, consider the following proof term:

\DM{
\AR{
\hole{\vA}{\Set}\SC\hole{\vk}{\Nat}\SC
\hole{\vx}{\vA}\SC\hole{\vxs}{\Vect\:\vA\:\vk}\SC
\\
\hole{\vys}{\Vect\:\vA\:(\suc\:\vk)}\SC\vys
}
}

\noindent
If $\vx$ is in focus (i.e., at the head of the hole queue) and we attempt to
$\MO{Fill}$ it with an $\TC{Int}$ value $42$, we have:

\begin{itemize}
\item $\Check\:42\:\mq\:\TC{Int}$
\item Unifying $\TC{Int}$ with $\vA$ (the type of $\vx$) is only possible if
$\vA\:=\:\TC{Int}$, so we solve $\vA$.
\end{itemize}

\noindent
Therefore the resulting proof term is:

\DM{
\AR{
\hole{\vk}{\Nat}\SC
\guess{\vx}{\TC{Int}}{42}\SC\hole{\vxs}{\Vect\:\TC{Int}\:\vk}\SC
\\
\hole{\vys}{\Vect\:\TC{Int}\:(\suc\:\vk)}\SC\vys
}
}

The $\MO{Solve}$ tactic operates on a guess binding. If the guess is \remph{pure}, i.e., it
is a \TT{} term containing no hole or guess bindings, then the value attached to
the guess is substituted into its scope:

\DM{
\PA{\A}{
\MO{Solve}_\Gamma & (\guess{\vx}{\vT}{\vv}\SC\ve) &
   \MoRet{\RW{return}\:\ve[\vv/\vx]\hg\mbox{(if $\MO{Pure}\:\vv$)}}
}
}

In each of these tactics, if any step fails, or the term in focus does not take
the correct form (e.g. is not a guess in the case of $\MO{Solve}$ or not a hole
in the case of $\MO{Claim}$ and $\MO{Fill}$, the entire tactic fails. We can
handle failure using the $\MO{Try}$ tactic combinator:

\DM{
\PA{\A\A\A}{
\MO{Try}_\Gamma & \VV{t1} & \VV{t2} & \vt &
   \MoRet{\AR{\VV{t1}_\Gamma\:\vt\hg\mbox{(if $\VV{t1}$ succeeds)} \\
              \VV{t2}_\Gamma\:\vt\hg\mbox{(otherwise)}}}
}
}

\subsubsection{Creating binders}

We also define primitive tactics for constructing binders. We can create a $\lambda$
binding if the goal normalises to a function type:

\DM{
\PA{\A\A}{
\MO{Lambda}_\Gamma & \vn & (\hole{\vx}{\vT}\SC\vx) &
 \MoRet{\RW{do}\:\AR{
   \all{\vy}{\vS}\SC\vT'\:\leftarrow\:\Eval\:\vT \\
   \RW{return}\:\lam{\vn}{\vS}\SC\hole{\vx}{\vT'[\vn/\vy]}\SC\vx
   }
   }
}
}

\noindent
We can create a $\forall$ binding if the goal is a $\TC{Set}$:

\DM{
\PA{\A\A}{
\MO{Pi}_\Gamma & (\vn\Hab\vS) & (\hole{\vx}{\Set}\SC\vx) &
 \MoRet{\RW{do}\:\AR{
   \Set\:\leftarrow\:\Check\:\vS\\
   \RW{return}\:\all{\vn}{\vS}\SC\hole{\vx}{\Set}\SC\vx
   }
   }
}
}

\noindent
To create a $\LET$ binding, we give a type and a value.

\DM{
\PA{\A\A}{
\MO{Let}_\Gamma & (\vn\Hab\vS\defq\vv) & (\hole{\vx}{\vT}\SC\vx) &
 \MoRet{\RW{do}\:\AR{
   \Set\:\leftarrow\:\Check\:\vS\\
   \vS'\:\leftarrow\:\Check\:\vv\\
   \Unify\:\vS\:\vS'\\
   \RW{return}\:\LET\:\vn\Hab\vS\defq\vv\SC\hole{\vx}{\vT}\SC\vx
   }
   }
}
}

Each of these tactics require the term in focus to be of the form $\hole{\vx}{\vT}\SC\vx$.
This is important, because if the scope of the binding was an arbitrary expression $\ve$,
the binder would be scoped across this whole expression rather than the subexpression
$\vx$ as intended.
The $\MO{attack}$ tactic ensures that a hole
is in the appropriate form, creating a new hole $\vh$ which is placed at the head
of the queue:

\DM{
\PA{\A}{
\MO{Attack}_\Gamma & (\hole{\vx}{\vT}\SC\ve) &
 \MoRet{\RW{return}\:\guess{\vx}{\vT}{(\hole{\vh}{\vT}\SC\vh)}\SC\ve}
}
}

Finally, we can convert a hole binding to a pattern binding by giving the 
pattern variable a name. This solves a hole
by adding the pattern binding to the proof state, and updating the proof term
with the pattern variable directly:

\DM{
\PA{\A\A}{
\MO{Pat}_\Gamma & \vn & (\hole{\vx}{\vT}\SC\ve) &
  \MoRet{\RW{do}\:\AR{
    \MO{PatBind}\:(\vx\Hab\vT)\\
    \RW{return}\:\ve[\vn/\vx]
  }}
}
}

The $\MO{PatBind}$ operation simply updates the proof state with the given pattern
binding. Once we have created bindings from the left hand side of a pattern
matching definition, for example, we can retain these bindings for use when
building the right hand side.

\subsubsection{Example}

Tactics are executed by a higher level meta-operation $\MO{RunTac}$, which
locates the appropriate sub-term, applies the tactic with the context
local to this sub-term, and
replaces the sub-term with the term returned by the
tactic. It then updates the hole queue in the proof state, and updates holes which have
been solved by unification. If the tactic creates new holes, these are automatically
added to the \remph{head} of the hole queue.
For example, consider the following simple
\TT{} definition for the identify function:

\DM{
\AR{
\FN{id}\Hab\all{\vA}{\Set}\SC\all{\va}{\vA}\SC\vA \\
\FN{id}\:=\:\lam{\vA}{\Set}\SC\lam{\va}{\vA}\SC\va  
}
}

We can build $\FN{id}$ either as a complete term, or by applying a sequence of tactics.
To achieve this, we create a proof state initialised with the type of $\FN{id}$ and
apply a series of $\MO{Lambda}$ and $\MO{Fill}$ operations using $\MO{RunTac}$:

\DM{
\AR{
\MO{MkId}\:\mq\:\RW{do}\:
 \AR{
   \MO{NewProof}\:\all{\vA}{\Set}\SC\all{\va}{\vA}\SC\vA \\
   \MO{RunTac}\;\MO{Attack} \\
   \MO{RunTac}\;(\MO{Lambda}\;\vA) \\
   \MO{RunTac}\;\MO{Attack} \\
   \MO{RunTac}\;(\MO{Lambda}\;\va) \\
   \MO{RunTac}\;(\MO{Fill}\;\va)\\ 
   \MO{RunTac}\;\MO{Solve}\\
   \MO{RunTac}\;\MO{Solve}\\
   \MO{RunTac}\;\MO{Solve}\\
 }
}
}

To aid readability, we will elide $\MO{RunTac}$, and use a semi-colon to indicate
sequencing. Using this convention, we can build $\FN{id}$'s type and definition as shown
in Figure \ref{idelab}. Note that $\MO{Term}$ retrieves the proof term from the current proof
state. Both $\MO{MkIdType}$ and $\MO{MkId}$ finish by returning a completed \TT{} term.
Note in particular that each $\MO{Attack}$ and each $\MO{Fill}$, which create new guesses,
are closed with a $\MO{Solve}$.

Setting up elaboration in this way, with a proof state captured in a monad,
and a primitive collection of tactics,
makes it easy to derive more complex tactics for elaborating higher level language constructs,
in much the same way as the \texttt{Ltac} language in Coq. As a result, the
description of elaboration of
a language construct (or a program such as $\FN{id}$) bears a strong resemblance to
a Coq proof script.

\FFIG{
\AR{
\MO{MkIdType}\:\mq\:\RW{do}\;
 \AR{
   \MO{NewProof}\:\Set\\
   \MO{Attack} ; \MO{Pi}\:(\vA\Hab\Set) ;
   \MO{Attack} ; \MO{Pi}\:(\va\Hab\vA) \\
   \MO{Fill}\;\vA \\
   \MO{Solve} ; \MO{Solve} ; \MO{Solve} \\
   \MO{Term}
 }
 \medskip\\
\MO{MkId}\:\mq\:\RW{do}\;
 \AR{
   \vt\:\leftarrow\:\MO{MkIdType}; \MO{NewProof}\:\vt\\
   \MO{Attack} ; \MO{Lambda}\;\vA ; 
   \MO{Attack} ; \MO{Lambda}\;\va \\
   \MO{Fill}\;\va \\
   \MO{Solve} ; \MO{Solve} ; \MO{Solve}\\
   \MO{Term}
 }
}
}
{Building $\FN{id}$ with tactics}
{idelab}

%--- give unify in full, esp. as it solves sub goals? Maybe...

% Unify' G x t             = Success (x, t) if ?x : t in G
% Unify' G t x             = Success (x, t) if ?x : t in G
% Unify' G (b x. e) (b' x'. e')   = Unify' G b b'; Unify' G;b e e'[x/x']
% Unify' G ((\x.e) x) e'   = Unify' G e e' 
% Unify' G e ((\x.e') x)   = Unify' G e e' 
% Unify' G (f es) (f' es') = vs <- Unify' G f f'; Injective f
                             
% Unify' G x y             = Success () if G |- x == y
% Unify' G . .             = Failure

% Unify' G (\x : t . e) (\x : t' . e') = Unify' G t t'; Unify' G e e'
% ...


%\DM{
%}

\subsection{Elaborating Expressions}

\newcommand{\piimp}[2]{\mbox{\texttt{\{ $#1$ : $#2$ \} -> }}}
%\mathtt{\{}#1\mathtt{:}#2\mbox{\texttt{\} -> }}}
\newcommand{\piexp}[2]{\mbox{\texttt{( $#1$ : $#2$ ) -> }}}
%\newcommand{\piexp}[2]{\mathtt{(}#1\mathtt{:}#2\mbox{\texttt{) -> }}}
\newcommand{\piconst}[1]{\mbox{\texttt{$#1$ => }}}
\newcommand{\icase}{\mathtt{case}}
\newcommand{\iwith}{\mathtt{with}}
\newcommand{\idata}{\mathtt{data}}
\newcommand{\iclass}{\mathtt{class}}
\newcommand{\iinstance}{\mathtt{instance}}
\newcommand{\iwhere}{\mathtt{where}}
\newcommand{\iof}{\mathtt{of}}
\newcommand{\ilet}[2]{\mathtt{let}\;#1\;\mathtt{=}\;#2\;\mathtt{in}}
\newcommand{\ilam}[1]{\mathtt{\backslash}\;#1\;\mathtt{=>}}
\newcommand{\iarg}[2]{\mbox{\texttt{\{ $#1$ = $#2$ \}}}}
\newcommand{\ihab}[2]{\mbox{\texttt{$#1$ : $#2$}}}
\newcommand{\carg}[1]{\mbox{\texttt{\{\{ $#1$ \}\}}}}
\newcommand{\fatarrow}{\mbox{\texttt{=>}}}
\newcommand{\ibar}{\mbox{\texttt{|}}}
\newcommand{\mvar}[1]{\mbox{\texttt{?}}#1}

\FFIG{
\AR{
\begin{array}{rll@{\hg}rll}
\ve ::= & \vc & (\mbox{constant}) &
\mid & \vx & (\mbox{variable}) \\
\mid & \vt & (\mbox{type}) &
\mid & \ve\:\ta & (\mbox{function application}) \\
\mid & \ilam{\vx}\:\ve & (\mbox{lambda abstraction}) &
\mid & \ilet{\vx}{\ve}\:\ve & (\mbox{let binding}) \\
%\mid & \icase\:\ve\:\iof\:\vec{\VV{alt}} & (\mbox{case expression}) &
%\mid & \mvar{\vx} & (\mbox{metavariable}) \\
\mid & \_ & (\mbox{placeholder})
\end{array}
\medskip\\
\begin{array}{rll}
\va :: = & \ve & (\mbox{normal argument}) \\
\mid & \iarg{\vx}{\ve} & (\mbox{implicit argument with value}) \\
\mid & \carg{\ve} & (\mbox{explicit class instance}) 
\medskip\\
\VV{alt} ::= & \ve\:\fatarrow\:\ve & (\mbox{case alternative})\\
\end{array}
\medskip\\
\begin{array}{rll}
\vt ::= & \ve & (\mbox{expression}) \\
\mid & \piimp{\vx}{\vt}\vt & (\mbox{implicit function space}) \\
\mid & \piexp{\vx}{\vt}\vt & (\mbox{explicit function space}) \\
\mid & \VV{constr}\;\vt & (\mbox{constrained type}) \\
\end{array}
\medskip\\
\begin{array}{rll}
\VV{constr} ::= & \piconst{\ttt} & (\mbox{type class constraint}) \\
\end{array}
}
}
{\IdrisM{} expressions}
{idrism}

\FFIG{
\AR{
\begin{array}{rll}
\vd ::= & \ihab{\vx}{\vt} & (\mbox{type declaration}) \\
\mid & \VV{pclause} & (\mbox{pattern clause}) \\
\mid & \VV{ddecl} & (\mbox{data type declaration}) \\
\mid & \VV{cdecl} & (\mbox{class declaration}) \\
\mid & \VV{idecl} & (\mbox{instance declaration}) \\
\end{array}
\medskip\\
\begin{array}{ll}
\begin{array}{rll}
\VV{pclause} ::= & 
\vx\:\ttt\:[\ibar\;\te]\mbox{\texttt{ = }}\ve \hg[\iwhere\;\td]\\ 
\mid & \AR{
\vx\:\ttt\:[\ibar\;\te]\;\iwith\;\ve\\
\hg\vec{\VV{pclause}}
}
\end{array}
&
\begin{array}{rll}
\VV{ddecl} ::= & \idata\;\ihab{\vx}{\vt}\;\iwhere\;\vec{\VV{con}}\\
\VV{con} ::= & \ihab{\vx}{\vt}\\
\medskip\\
\VV{cdecl} ::= & \iclass\;[\VV{constr}]\;\vx\;(\ihab{\tx}{\ttt})\;\iwhere\;\td
\\
\VV{idecl} ::= & \iinstance\:[\VV{constr}]\;\vx\;\vt\;\iwhere\;\td
\end{array}
\end{array}
}
}
{\IdrisM{} declarations}
{idrismd}

\IdrisM{}, a subset of \Idris{} not including syntactic sugar (e.g. pairs, do notation, 
infix operators, etc),
and with implicit types written in full.
Extensions over \TT{}: implicit syntax, case expressions. Functions are applied to
multiple arguments, rather than one at a time (this helps with implicit syntax).
\IdrisM{} also supports explicit \demph{metavariables}. A metavariable \texttt{?mvar}
creates a new global function \tFN{mvar}, with its type such that it would
be type correct to apply \tFN{mvar} to all of the variables in scope.

Implicit and type class arguments? Expanded at the application site (we need to know
it's the global name after all and we do that by type).

We define a meta-operation $\ttinterp{\cdot}$ which runs relative to a proof state.
Its effect is to update the proof state so that the hole in focus contains a representation
of the given expression, by applying tactics. We assume that the proof
state has already been set up, which means that we always know the type of the
expression we are building.

\subsubsection{Elaborating variables and constants}

In the simplest cases, there is a direct translation from an \IdrisM{} expression to
a \TT{} expression --- we build \TT{} representations of variables and constants using
the \MO{Fill} tactic:

\DM{
\AR{
\ttinterp{\vx}\:\mq\:\RW{do}\:\MO{Fill}\:\vx;\:\MO{Solve}\\
\ttinterp{\vc}\:\mq\:\RW{do}\:\MO{Fill}\:\vc;\:\MO{Solve}\\
}
}

We need not concern ourselves with type checking variables or constants here --- $\MO{Fill}$
will handle this, type checking $\vx$ or $\vc$ and unifying the result with the
hole type. If there are any errors, elaboration will fail.

If we are building the left hand side of a pattern clause, however, there is a problem,
as it is the left hand side which \remph{defines} variables. In this context, we assume
that attempting to elaborate a variable which does not type check means that the variable
is a pattern variable:

\DM{
\ttinterp{\vx}\:\mq\:
\MO{Try}\:\AR{(\RW{do}\:\MO{Fill}\:\vx;\:\MO{Solve})\\
(\MO{Pat}\:\vx)\hg\hg\mbox{(If elaborating a left hand side)}}
}

We also need to elaborate \remph{placeholders}, which are subexpressions we expect to
solve by unification. In this case, we simply move on to the next hole in the queue, moving
the current hole to the end of the queue with $\MO{Unfocus}$:

\DM{
\ttinterp{\_}\:\mq\:\MO{Unfocus}
}

On encountering a placeholder, our assumption is that unification will eventually solve
the hole.

\subsubsection{Elaborating bindings}

To elaborate a $\lambda$ binding, we $\MO{Attack}$ the hole, which must be a function type
or elaboration will fail,
apply the $\MO{Lambda}$ tactic, elaborate the scope, then $\MO{Solve}$, which discharges
the $\MO{Attack}$:

\DM{
\ttinterp{\ilam{\vx}\ve}\:\mq\:
\RW{do}\:
\AR{
\MO{Attack};\;\MO{Lambda}\:\vx\\
\ttinterp{\ve}\\
\MO{Solve}
}
}

Note that there is no type on the $\lambda$-binding in \IdrisM{}. There is no need --- since
elaboration is type directed, the $\MO{Lambda}$ tactic finds the type of the binding by
looking at the type of the hole. In full \Idris{}, types are allowed on bindings, and the
elaborator merely checks that the given type is equivalent to the inferred type.

Elaborating a function type is more tricky, since we have to elaborate the argument
type (itself an \IdrisM{} expression) then elaborate the scope. To achieve this, we
create a new goal $\vX$ for the argument type $\vt$, where $\vX$ is a fresh name,
and introduce a function binding with argument type $\vX$. We can then focus on
$\vX$, and elaborate it with the expression $\vt$. Finally, we elaborate the
scope of the binding.

\DM{
\AR{
\ttinterp{\piexp{\vx}{\vt}\ve}\:\mq\:
\RW{do}\:
\AR{
\MO{Attack};\;\MO{Claim}\:(\vX\Hab\Set)\\
\MO{Pi}\:(\vx\Hab\vX)\\
\MO{Focus}\:\vX\\
\ttinterp{\vt};\;
\ttinterp{\ve}\\
\MO{Solve}
}
}
}

Elaborating $\vt$ will involve solving unification problems, which will, if the
program is type correct, solve $\vX$. After focussing on $\vX$ and elaborating
$\vt$, there is no need to refocus on the hole representing the scope, as it was
previously at the head of the hole queue before focussing on $\vX$.
Elaboration of implicit and constraint
argument types is exactly the same --- \TT{} makes no distinction between then.

To elaborate a \texttt{let} binding, we take a similar approach, creating new subgoals
for the \texttt{let}-bound value and its type, then elaborating the scope. Again,
if elaborating the scope is successful, unification will solve the claimed variables
$\vV$ and $\vX$.

\DM{
\ttinterp{\ilet{\vx}{\vv}\:{\ve}}\:\mq\:\RW{do}\:
\AR{
\MO{Attack};\;
\MO{Claim}\:(\vX\Hab\Set);\; 
\MO{Claim}\:(\vV\Hab\vX)\\
\MO{Let}\:(\vx\Hab\vX\defq\vV)\\
\MO{Focus}\:\vV\\
\ttinterp{\vv};\;
\ttinterp{\ve}
}
}
\subsubsection{Elaborating applications}

There are two cases to consider when elaborating applications:

\begin{itemize}
\item Applying a global function name to arguments, some of which may be implicit.
\item Applying an expression, which does not have implicit arguments, to an argument.
\end{itemize}

In the first case, we must expand the application to include implicit arguments.
For example \texttt{vAdd xs ys} is expanded to
\texttt{vAdd \{a=\_\} \{n=\_\} \{\{\_\}\} xs ys}, adding the implicit arguments
\texttt{a} and \texttt{n} and a type class instance argument. The meta-operation
$\MO{Expand}\:\vx\:\ta$, given a global function name $\vx$ and the
arguments supplied by the programmer $\ta$, returns a new argument list
$\ta'$ with implicit and type class arguments added. Each value in
$\ta'$ is paired with an explicit name for the argument.

Implicit arguments are solved by unification
--- the type or value of another argument determines the value of an implicit argument,
with the appeal to $\MO{Unify}$ in the $\MO{Fill}$ tactic solving as many extra holes
as it can. However, unification problems can take several forms. For example, assuming
$\vf$, $\vg$, $\vx$ are holes, we might have unification problems of the following
forms:

\DM{
\AR{
\MO{Unify}_\Gamma\;\vf\:\TC{Int}\\
\MO{Unify}_\Gamma\;(\vf\:\vx)\:\TC{Int}\\
\MO{Unify}_\Gamma\;(\vf\:\vx)\:(\vg\:\vx)\\
}
}

The first problem has a solution: $\vf=\TC{Int}$. The second and third problems have
no solution without further information. In the second case, we cannot conclude 
anything about $\vf$ or $\vx$ just from knowing that $\vf\:\vx\:=\:\TC{Int}$.
In the third case, although we have $\vx\:=\:\vx$ in argument position, we cannot
conclude that $\vf\:=\:\vg$ unless we know that the function which solves $\vf$ or
$\vg$ is injective.

Once $\vf$ or $\vg$ is solved in some other way, perhaps by being given explicitly
in another argument, these unification problems can make progress. Otherwise, unification
fails. As a result, the order in which functions and arguments are
elaborated matters. Sometimes, we may learn more by elaborating a function first
and the arguments later, sometimes it may be the other way around. 

One option when unification fails is to store the problem (i.e. the expressions to be
unified with their local context) and refine it with further information when it becomes 
available. In practice, we have found a simpler approach to be effective: try
elaborating the function first, then arguments. If that fails, try elaborating the
arguments first.

Elaborating a global function application makes a $\MO{Claim}$ for each argument
in turn. Then the function is elaborated, applied to each of the claimed arguments.
Finally, each argument which has been given explicitly is elaborated. If this fails,
elaboration tries again in the opposite order. Finally, we resolve any type
class instance arguments with the built-in $\MO{Instance}$ tactic.

\DM{
\ttinterp{\vx\:\ta}\:\mq\:
\AR{
\RW{do}\:
\AR{
(\tn,\tv)\:\gets\:\MO{Expand}\:\vx\:\ta\\
\vec{\MO{Claim}}\:(\tn\Hab\tT)\\
\MO{Try}\:\AR{
(\RW{do}\:\AR{
\MO{Fill}\:\vx\:\tn\\
\vec{\MO{Focus}}\:\tn;\:\ttinterp{\tv}\hg\mbox{(for non-placeholder $\vv$)}\\
\MO{Solve})
}\\
(\RW{do}\:\AR{
\vec{\MO{Focus}}\:\tn;\:\ttinterp{\tv}\hg\mbox{(for non-placeholder $\vv$)}\\
\MO{Fill}\:\vx\:\tn\\
\MO{Solve})
}}
\\
\vec{\MO{Instance}}\:\tn\hg\mbox{(for type class constraint argument $\vn$)}
}
}
}

The $\MO{Instance}$ tactic focuses on the given hole and searches the context for
a type class instance which would solve the goal directly. First, it examines the local
context, then recursively searches the global context.

To elaborate a simple function application, of an arbitrary expression to an arbitrary
argument, we need not worry about implicit arguments or class constraints. Instead,
we simply elaborate the function and argument, and apply the results. Since elaboration
is type directed, however, we do need to construct an appropriate type for the function.
As before, we try elaboration in two orders, as we may learn something from elaborating
the function or argument which helps with unification:

\DM{
\ttinterp{\ve\:\va}\:\mq\:\RW{do}\:
\AR{
\MO{Claim}\:(\vA\Hab\Set);\;\MO{Claim}\:(\vB\Hab\Set)\\
\MO{Claim}\:(\vf\Hab\vA\to\vB);\;\MO{Claim}\:(\vs\Hab\vA)\\
\MO{Try}\:
\AR{
(\RW{do}\:\AR{
\MO{Focus}\:\vf;\;\ttinterp{\ve}\\
\MO{Focus}\:\vs;\;\ttinterp{\va})}\\
(\RW{do}\:\AR{
\MO{Focus}\:\vs;\;\ttinterp{\va}\\
\MO{Focus}\:\vf;\;\ttinterp{\ve})}\\
}
}
}

%\subsubsection{Elaborating metavariables}

%\subsubsection{Elaborating \texttt{case} expressions}

\subsection{Elaborating Declarations}

We can use $\ttinterp{\cdot}$ to
build the top level $\MO{Elab}$ operation, which translates \IdrisM{} type, function
and class declarations into \TT{} declarations.

\newcommand{\edo}[1]{\RW{do}\:\AR{#1}}

\subsubsection{Elaborating Type Declarations}

To elaborate a type declaration, we simply create a new proof state,
translate the \IdrisM{} type to a \TT{} type, then add the resulting type
as a \TT{} declaration:

\DM{
\MO{Elab}\:(\vx\Hab\vt)\:\mq
\:\edo{\MO{NewProof}\:\Set\\
       \ttinterp{\vt}\\
       \vt'\gets\MO{Term}\\
       \MO{TTDecl}\:(\vx\Hab\vt')
}}

The final $\MO{TTDecl}$ takes the result of elaboration, type checks it,
and adds it to the global context if type checking succeeds. This final type check
ensures that the elaboration process does not allow any ill-typed terms to creep
into the context.

\subsubsection{Elaborating Data Types}

To elaborate a data declaration, we need to elaborate the type declaration itself,
as a normal type declaration. This ensures that the type is in scope when
elaborating the constructor types. We then use the results to build a \TT{} data
declaration.

\DM{
\AR{
\MO{Elab}\:(\idata\;\ihab{\vx}{\vt}\;\iwhere\;\tc)\\
\hg\hg\mq\:\edo{\MO{NewProof}\:\Set\\
                \ttinterp{\vt}\\
                \vt'\gets\MO{Term}\\
                \MO{TTDecl}\:(\vx\Hab\vt')\\
                \tc'\gets\vec{\MO{ElabCon}}\:\tc\\
                \MO{TTDecl}\:(\Data\:\vx\Hab\vt'\:\Where\:\tc')
}
\AR{
\MO{ElabCon}\:(\vx\Hab\vt)\\
\hg\hg\mq\: \edo{\MO{NewProof}\:\Set\\
                 \ttinterp{\vt}\\
                 \vt'\gets\MO{Term}\\
                 \RW{return}\:(\vx\Hab\vt')
}
}}}

\subsubsection{Elaborating Pattern Matching}

Work clause by clause, $\MO{ElabClause}$ returns a left and right hand side, and
may have the side effect of adding things to the context, such as definitions in
$\iwhere$ clauses. Collect together in a full definition at the end. 

In the simplest case, no $\iwhere$ clause:

\DM{
\MO{ElabClause}\:(\vx\:\ttt\:=\:\ve)\:\mq\:?
}

How do we elaborate the left hand side, given that elaboration is type directed, and
we do not know its type until after we have elaborated it? The easiest way, which
requires no change to the elaborator, is to define a type $\TC{Infer}$:

\DM{
\Data\hg\TC{Infer}\Hab\Set\hg\Where\hg\DC{MkInfer}\Hab\all{\va}{\Set}\SC\va\to\TC{Infer}
}

Now to elaborate the left hand side, we elaborate $\DC{MkInfer}\:\_\:(\vx\:\ttt)$ and
extract the value and unified type when complete.

\DM{
\AR{
\MO{ElabClause}\:(\vx\:\ttt\:=\:\ve)\:\mq\:\\
\hg\hg\edo{
\MO{NewProof}\:\TC{Infer};\;
\ttinterp{\DC{MkInfer}\:\_\:(\vx\:\ttt)}\\
\DC{MkInfer}\:\vT\:\VV{lhs}\gets\MO{Term}\\
\tp\gets\MO{Patterns}\\
\MO{NewTerm}\:\vT;\;
\ttinterp{\ve}\\
\VV{rhs}\gets\MO{Term}\\
\RW{return}\:(\RW{var}\:\tp\SC\VV{lhs}\:=\:\VV{rhs})
}
}
}

This infers a type for the left hand side, creating pattern variable bindings. Then, 
it elaborates the right hand side using the inferred type of the left hand side
and the inferred pattern bindings, which are retrieved from the state with the
$\MO{Patterns}$ operation. Finally, it returns a pattern clause in \TT{} form.
To elaborate a collection of pattern clauses, we simply map $\MO{ElabClause}$ over
the clauses and add the resulting collection to \TT{}.

\DM{
\AR{
\MO{Elab}\:\vec{\VV{pclause}}\:\mq\:
\edo{
\tc\gets\vec{\MO{ElabClause}}\:\vec{\VV{pclause}}\\
\MO{TTDecl}\:\tc
}
}
}

Elaborating a clause is made only slightly more complex by the presence of
$\iwhere$ clauses. In this case, we elaborate the left hand side, add the pattern
bound variables as extra arguments to the declarations in the $\iwhere$ block,
and recursively elaborate before elaborating the right hand side.

\DM{
\AR{
\MO{ElabClause}\:(\vx\:\ttt\:=\:\ve\:\iwhere\:\td)\:\mq\:\\
\hg\hg\edo{
\MO{NewProof}\:\TC{Infer};\;
\ttinterp{\DC{MkInfer}\:\_\:(\vx\:\ttt)}\\
\DC{MkInfer}\:\vT\:\VV{lhs}\gets\MO{Term}\\
\tp\gets\MO{Patterns}\\
(\td', \ve') \gets\MO{Lift}\:\tp\:\td\:\ve\\
\vec{\MO{Elab}}\:\td'\\
\MO{NewTerm}\:\vT;\;
\ttinterp{\ve'}\\
\VV{rhs}\gets\MO{Term}\\
\RW{return}\:(\RW{var}\:\tp\SC\VV{lhs}\:=\:\VV{rhs})
}
}
}

In \TT{}, all definitions must be at the top level. Therefore, the declarations 
in the $\iwhere$ block are lifted out, adding the pattern bound names as additional arguments
to ensure they are in scope, using the $\MO{Lift}$ operation. This also modifies
the right hand side $\ve$ to use the lifted definitions, rather than the original.

\subsubsection{Elaborating the \texttt{with} rule}

The \texttt{with} rule allows \remph{dependent} pattern matching on intermediate values.
To translate this into \TT{}, we need to construct an auxiliary top level
definition for 
the intermediate pattern match. For example, recall \tFN{natToBin}:

\useverb{natToBin}

\noindent
This is equivalent to the following, using top level definitions only:

\begin{SaveVerbatim}{natToBinw}

natToBin : Nat -> List Bool
natToBin O = Nil
natToBin k = natToBin' k (parity k)

\end{SaveVerbatim}
\begin{SaveVerbatim}{natToBinwp}

natToBin' : (n : Nat) -> Parity n -> List Bool
natToBin' (j + j)     even = False :: natToBin j
natToBin' (S (j + j)) odd  = True  :: natToBin j

\end{SaveVerbatim}
\useverb{natToBinw}

\useverb{natToBinwp}

The difference when using the \texttt{with} rule is that there is no need for
the explicit type declaration for the auxiliary function \texttt{natToBin}.
Elaboration of \texttt{with} builds the type declaration and auxiliary function
automatically:


\DM{
\AR{
\MO{ElabClause}\:(\vx\:\ttt\:\iwith\:\ve\:\vec{\VV{pclause}})\:\mq\\
\hg\hg\edo{
\MO{NewProof}\:\TC{Infer};\;
\ttinterp{\DC{MkInfer}\:\_\:(\vx\:\ttt)}\\
\DC{MkInfer}\:\vT\:\VV{lhs}\gets\MO{Term}\\
\tp\gets\MO{Patterns}\\
\MO{NewTerm}\:\TC{Infer};\;
\ttinterp{\DC{MkInfer}\:\_\:\ve}\\
\DC{MkInfer}\:\vW\:\vw'\gets\MO{Term}\\
\MO{TTDecl}\:(\vx'\Hab\tp\to\vW\to\vT)\\
\tc'\gets\vec{\MO{MatchWith}}\:\tp\:(\vx\:\ttt)\:\vec{\VV{pclause}}\\
\vec{\MO{Elab}}\:\tc'\\
\RW{return}\:(\RW{var}\:\tp\SC\VV{lhs}\:=\:\vx'\:\tp\:\vw')
}
\medskip\\
\MO{MatchWith}\:\tp\:(\vx\:\ttt)\:(\vx\:\tw\:\mid\:\ve\:\VV{rhs})\:\mq\\
\hg\hg\edo{
\tp'\gets\vec{\MO{Match}}\:\tw\:\ttt\\
\MO{CheckComplete}\:\tp\:\tp'\\
\RW{return}\:(\vx'\:\tp'\:\ve\:\VV{rhs})
}
}
}

A $\iwith$ block in a clause for $\vx$
results in an auxiliary definition $\vx'$, where $\vx'$ is a fresh name,
which takes an extra argument corresponding to the intermediate result which
is to be matched.
$\MO{MatchWith}$ matches the left hand side of the top level definition against
each $\VV{pclause}$, using the result of the match to build each pattern clause
for $\vx'$. 
The clauses in the with block must be of a specific form: they must have an initial
set of arguments $\tw$ which matches the outer clause arguments
$\ttt$, and an extra argument
$\ve$ which is the same type as the scrutinee of the \texttt{with}, $\vW$.
The right hand side, $\VV{rhs}$, may either return a value directly, containing 
\texttt{where} clauses, or be a nested \texttt{with} block. In any case, it
remains unchanged.
$\MO{Match}\:\vw\:\vt$ returns a list of pattern bindings containing
the variables in $\vt$ and their matches in $\vw$.
If the \texttt{with} block is well-formed, then
the resulting set of patterns $\tp'$ contains exactly the same pattern variables
as $\tp$, which is verified by $\MO{CheckComplete}$.


\subsubsection{Elaborating Class and Instance Declarations}

\subsection{Syntax Extensions}

\subsubsection{Metavariables}

\subsubsection{\texttt{case} expressions}

\subsubsection{Pairs and Dependent Pairs}

